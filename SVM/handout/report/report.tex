\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm2e}
\usepackage[pdftex]{hyperref}
\usepackage{listings}
\input{listing_setting.tex}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Data Mining: Learning from Large Data Sets - Fall Semester 2015}
\author{fgmehlin@student.ethz.ch\\ matteopo@student.ethz.ch\\ piusv@student.ethz.ch\\}
\date{\today}

\begin{document}
\maketitle

\section*{Large Scale Image Classification} 
This project consists of training a model for classification of two images, namely : Nature and People. From a set of extracted feature, we applied made use of a Stochastic Gradient Descent classifier.
\section{Mapper}

The mapper is divided into two subsequent parts :
\begin{enumerate}
\item Feature transformation
\item Classification with Cross-validation
\end{enumerate}

1.) The method transforms applies the following transformation on the sample features :
\begin{itemize}
\item $\widetilde{x_1} = \sqrt{|x|}$
\item $\widetilde{x_2} = cosh((\frac{\pi}{2})*x) - 1$
\item $\widetilde{x_3} = sin((\frac{\pi}{2})*x)$
\end{itemize}

\textbf{NOTE}: We also tried to implement Random Fourier Feature transformation but they gave worse results that the aforementioned transformations.

2.) We use a Stochastic Gradient Descent Classifier algorithm with l1 regularizer provided by the sklearn library. In order to make a justified choice of the several parameters we can set in the configuration of the algorithm, we use an estimation of the out-of-sample accuracy of the algorithm. To obtain this estimation, every time we run the classifier we randomly split the provided data set into a training set (80\% of the dataset) and a smaller test set (20\%). We then compute the classifier on the training set and evaluate it on the test set; by doing this we get an estimate of the out-of-sample accuracy of the algorithm.

Finally the mapper will output the tuple (1, feature\_weights) for the learned dataset.

\section{Reducer}

The reducer aggregates the weights from the different mappers to produce the final feature weights.

\section{Participation}



\end{document} 

